{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h1 style=“font-size:30px;“>Gradient Descent Assignment</h1>\n",
    "\n",
    "The previous section on Gradient Descent focused on how to implement the gradient calculation and weight update for a single variable ( `m` ), using simple math operations. In this assignment, you do the same, but for 2 variables.  \n",
    "\n",
    "We will use the full form of a line i.e. `y = mx + c`.  \n",
    "You need to estimate the values of two variables `m` and `c`, using Stochastic Gradient Descent.\n",
    "\n",
    "\n",
    "Tasks to implement for the 2 variables:   \n",
    "1. Implement the gradient calculation step for the 2 variables.\n",
    "2. Implement the weight update step for the 2 variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Maximum Points: 30\n",
    "\n",
    "<div>\n",
    "    <table>\n",
    "        <tr><td><h3>Section</h3></td> <td><h3>Problem</h3></td> <td><h3>Points</h3></td> </tr>\n",
    "        <tr><td><h3>2.1</h3></td> <td><h3>Implement Gradients </h3></td> <td><h3>20</h3></td> </tr>\n",
    "        <tr><td><h3>2.2</h3></td> <td><h3>Implement SGD</h3></td> <td><h3>10</h3></td> </tr>\n",
    "    </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T09:18:24.438409Z",
     "start_time": "2021-12-22T09:18:18.521229Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For reproducibility\n",
    "tf.random.set_seed(41)\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## 1 Generate Sample Data\n",
    "\n",
    "Here we define a function that will generate some sample data based on a **linear model** in the presence of random noise. We will generate 1,000 data points for this experiment. The independent variable, `x`, has values randomly distributed between -5 to 5. Values for `m` and `c` have been specified to create the data points for the dependent variable (`y`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T09:18:27.930244Z",
     "start_time": "2021-12-22T09:18:24.441410Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Generating y = mx + c + random noise.\n",
    "num_data = 1000\n",
    "\n",
    "# True values of m and c\n",
    "m_line = 3.3\n",
    "c_line = 5.3\n",
    "\n",
    "\n",
    "# Input (Generate random data between [-5,5]).\n",
    "x = tf.random.uniform([num_data], minval=-5, maxval=5)\n",
    "\n",
    "# Output (Generate data assuming y = mx + c + noise).\n",
    "y_label = m_line * x + c_line + tf.random.normal(x.shape).numpy()\n",
    "y = m_line * x + c_line\n",
    "\n",
    "# Plot the generated data points. \n",
    "plt.plot(x, y_label, '.', color='g', label=\"Data points\")\n",
    "plt.plot(x, y, color='b', label='y = mx + c', linewidth=3)\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "The goal is to find the \"unknown\" parameters ($m$ and $c$) of the linear model below so that we can predict $y$, given some value of $x$.  \n",
    "\n",
    "$$ y = mx + c $$  \n",
    "\n",
    "We have a set of data points $(x_i, y_i)$, and they should all satisfy the equation above. i.e.,\n",
    "\n",
    "$$ y_i = m x_i + c $$  \n",
    "\n",
    "Since the data is not perfectly linear due to the added noise, we can represent the **error** or a **residual** as follows: \n",
    "\n",
    "$$ e_i = (y_i - m x_i -c) $$   \n",
    "\n",
    "\n",
    "Next, we need to find a value of $m$ and $c$ that minimizes the error above. Positive or negative values of error are equally bad. So, we need to minimize the square of the above error, across all the data points.\n",
    "\n",
    "$$ l_{sse} = \\sum^N_{i=1}(y_i - m x_i -c)^2 \\\\ $$\n",
    "\n",
    "\n",
    "This form of the **loss function** is the sum of squared errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## 2 Gradient Descent [30 Points]\n",
    "\n",
    "\n",
    "We have already seen how the Math works for `m`. The same approach is used in the case of `m` and `c`.    \n",
    "We calculate the loss function and then take partial derivatives w.r.t `m` and `c` respectively. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "l &= \\sum^n_{i=1}(y_i - m x_i - c)^2 \\\\\n",
    "\\frac{\\partial l}{\\partial m}  &= -2 \\sum^n_{i=1} x_i(y_i - m x_i - c) \\\\\n",
    "\\frac{\\partial l}{\\partial c}  &= -2 \\sum^n_{i=1} (y_i - m x_i - c) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To follow the slope of the curve, we move `m` in the direction of negative gradient. However, we must control the rate at which we go down the slope, so that we do not overshoot the minimum. Thus, we use a parameter $\\lambda$ called the `learning rate`.\n",
    "$$\n",
    "\\begin{align}\n",
    "m_k &= m_{k-1} - \\lambda \\frac{\\partial l}{\\partial m} \\\\\n",
    "c_k &= c_{k-1} - \\lambda \\frac{\\partial l}{\\partial c} \\\\ \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "That is it! \n",
    "\n",
    "Let's implement this in code to see that it really works. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<!-- <div class=\"alert alert-block alert-info\">\n",
    "    <b>1. Implement Gradients: 20 Points</b>\n",
    "</div> -->\n",
    "\n",
    "### 2.1 Implement Gradients [20 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Useful tensorflow methods for this functions:**\n",
    "\n",
    "1. [reduce_sum](https://www.tensorflow.org/api_docs/python/tf/math/reduce_sum)\n",
    "2. [gather](https://www.tensorflow.org/api_docs/python/tf/gather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 6)\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "block_plot = False\n",
    "def plotting_1d(x, label):\n",
    "# Now plotting \n",
    "    plt.figure\n",
    "    plt.xlabel(label)\n",
    "    plt.plot(x.numpy(),'c--')\n",
    "    plt.title('Generic 1D plot')\n",
    "    plt.show(block=block_plot)\n",
    "\n",
    "def plotting_2d(x, x_axis_label, y, y_axis_label):\n",
    "    plt.figure\n",
    "    plt.xlabel(x_axis_label)\n",
    "    plt.ylabel(y_axis_label)\n",
    "    plt.plot(x.numpy(),y.numpy(),'c--')\n",
    "    plt.show(block=block_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T09:18:27.946247Z",
     "start_time": "2021-12-22T09:18:27.934239Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_wrt_m_and_c(inputs, labels, m, c, k):\n",
    "    \n",
    "    '''\n",
    "    All arguments are defined in the training section of this notebook. \n",
    "    This function will be called from the training section.  \n",
    "    So before completing this function go through the whole notebook.\n",
    "    \n",
    "    inputs (torch.tensor): input (X)\n",
    "    labels (torch.tensor): label (Y)\n",
    "    m (float): slope of the line\n",
    "    c (float): vertical intercept of line\n",
    "    k (torch.tensor, dtype=int): random index of data points\n",
    "    '''\n",
    "    num_iter0 = 970\n",
    "    lr0 = 0.005\n",
    "\n",
    "    # Data structures needed for computations\n",
    "    loss = tf.Variable(tf.zeros(shape=[num_iter0]))\n",
    "\n",
    "    # gradient w.r.t to m is g_m \n",
    "    # gradient w.r.t to c is g_c\n",
    "    \n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "    ###\n",
    "    # Start computing loss\n",
    "    for i in range(0, num_iter0):\n",
    "        pde_loss_m = -2 * tf.reduce_sum(inputs * (labels - m * inputs - c))/len(inputs)\n",
    "        pde_loss_c = -2 * tf.reduce_sum(labels - m * inputs - c)/len(inputs)\n",
    "        \n",
    "        # New slope and bias\n",
    "        m = m - lr0 * pde_loss_m\n",
    "        c = c - lr0 * pde_loss_c\n",
    "        \n",
    "        # Compute error\n",
    "        e = labels - m * inputs - c\n",
    " \n",
    "        \n",
    "        # Loss function based on the error\n",
    "        loss[i].assign(tf.reduce_sum(tf.multiply(e,e))/len(inputs))\n",
    "        print(i,loss[i].numpy())\n",
    "    \n",
    "    g_m = m.numpy()\n",
    "    c_m = c.numpy()\n",
    "    return g_m, c_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test your code before submitting it using the below code cell.**\n",
    "\n",
    "For the given input:\n",
    "```\n",
    "X = tf.convert_to_tensor([-0.0374,  2.6822, -4.1152])\n",
    "Y = tf.convert_to_tensor.tensor([ 5.1765, 14.1513, -8.2802])\n",
    "m = 2\n",
    "c = 3\n",
    "k = tf.convert_to_tensor.tensor([0, 2])\n",
    "```\n",
    "Output:\n",
    "```\n",
    "Gradient of m : -24.93\n",
    "Gradient of c : 1.60\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T09:18:27.993237Z",
     "start_time": "2021-12-22T09:18:27.953242Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 14.225357\n",
      "1 12.751525\n",
      "2 11.492106\n",
      "3 10.414134\n",
      "4 9.489747\n",
      "5 8.695395\n",
      "6 8.011172\n",
      "7 7.420258\n",
      "8 6.9084334\n",
      "9 6.463678\n",
      "10 6.075827\n",
      "11 5.736295\n",
      "12 5.4378147\n",
      "13 5.1742473\n",
      "14 4.9403987\n",
      "15 4.731876\n",
      "16 4.5449615\n",
      "17 4.376512\n",
      "18 4.223868\n",
      "19 4.084779\n",
      "20 3.9573348\n",
      "21 3.839922\n",
      "22 3.7311735\n",
      "23 3.6299274\n",
      "24 3.5352023\n",
      "25 3.4461634\n",
      "26 3.3621025\n",
      "27 3.2824156\n",
      "28 3.2065933\n",
      "29 3.1341991\n",
      "30 3.064861\n",
      "31 2.998264\n",
      "32 2.9341354\n",
      "33 2.872244\n",
      "34 2.8123915\n",
      "35 2.7544072\n",
      "36 2.6981437\n",
      "37 2.6434739\n",
      "38 2.5902882\n",
      "39 2.5384905\n",
      "40 2.4879973\n",
      "41 2.4387357\n",
      "42 2.3906422\n",
      "43 2.3436587\n",
      "44 2.2977383\n",
      "45 2.2528312\n",
      "46 2.2089016\n",
      "47 2.1659122\n",
      "48 2.1238277\n",
      "49 2.0826218\n",
      "50 2.0422647\n",
      "51 2.0027335\n",
      "52 1.9640015\n",
      "53 1.9260501\n",
      "54 1.8888563\n",
      "55 1.8524035\n",
      "56 1.8166718\n",
      "57 1.7816461\n",
      "58 1.7473074\n",
      "59 1.7136421\n",
      "60 1.6806349\n",
      "61 1.6482716\n",
      "62 1.6165372\n",
      "63 1.585419\n",
      "64 1.5549068\n",
      "65 1.5249844\n",
      "66 1.4956414\n",
      "67 1.466865\n",
      "68 1.438645\n",
      "69 1.4109703\n",
      "70 1.3838292\n",
      "71 1.357212\n",
      "72 1.3311075\n",
      "73 1.305506\n",
      "74 1.2803985\n",
      "75 1.255775\n",
      "76 1.2316247\n",
      "77 1.2079405\n",
      "78 1.1847118\n",
      "79 1.1619301\n",
      "80 1.1395869\n",
      "81 1.1176728\n",
      "82 1.0961813\n",
      "83 1.0751032\n",
      "84 1.0544298\n",
      "85 1.0341545\n",
      "86 1.0142697\n",
      "87 0.99476665\n",
      "88 0.97563934\n",
      "89 0.95687914\n",
      "90 0.93848014\n",
      "91 0.92043567\n",
      "92 0.90273696\n",
      "93 0.8853796\n",
      "94 0.8683555\n",
      "95 0.8516584\n",
      "96 0.8352825\n",
      "97 0.8192212\n",
      "98 0.8034685\n",
      "99 0.7880201\n",
      "100 0.77286863\n",
      "101 0.75800747\n",
      "102 0.743433\n",
      "103 0.72913766\n",
      "104 0.7151177\n",
      "105 0.7013679\n",
      "106 0.68788224\n",
      "107 0.6746561\n",
      "108 0.6616835\n",
      "109 0.64896107\n",
      "110 0.63648283\n",
      "111 0.6242445\n",
      "112 0.6122417\n",
      "113 0.6004692\n",
      "114 0.5889237\n",
      "115 0.57759935\n",
      "116 0.5664939\n",
      "117 0.55560184\n",
      "118 0.5449185\n",
      "119 0.5344407\n",
      "120 0.5241656\n",
      "121 0.5140867\n",
      "122 0.5042026\n",
      "123 0.49450743\n",
      "124 0.4849987\n",
      "125 0.47567376\n",
      "126 0.4665277\n",
      "127 0.4575571\n",
      "128 0.4487597\n",
      "129 0.4401314\n",
      "130 0.4316682\n",
      "131 0.42336854\n",
      "132 0.41522762\n",
      "133 0.40724382\n",
      "134 0.39941385\n",
      "135 0.39173377\n",
      "136 0.38420185\n",
      "137 0.37681448\n",
      "138 0.3695688\n",
      "139 0.36246228\n",
      "140 0.35549322\n",
      "141 0.3486582\n",
      "142 0.34195435\n",
      "143 0.33537886\n",
      "144 0.32893017\n",
      "145 0.32260585\n",
      "146 0.3164024\n",
      "147 0.31031856\n",
      "148 0.30435184\n",
      "149 0.29850018\n",
      "150 0.2927603\n",
      "151 0.28713146\n",
      "152 0.28161028\n",
      "153 0.27619562\n",
      "154 0.27088508\n",
      "155 0.2656765\n",
      "156 0.2605684\n",
      "157 0.25555772\n",
      "158 0.25064427\n",
      "159 0.24582498\n",
      "160 0.24109866\n",
      "161 0.2364624\n",
      "162 0.2319162\n",
      "163 0.22745681\n",
      "164 0.22308324\n",
      "165 0.21879399\n",
      "166 0.21458696\n",
      "167 0.21046083\n",
      "168 0.20641355\n",
      "169 0.20244539\n",
      "170 0.19855279\n",
      "171 0.19473474\n",
      "172 0.19099055\n",
      "173 0.18731798\n",
      "174 0.18371655\n",
      "175 0.18018395\n",
      "176 0.17671926\n",
      "177 0.17332156\n",
      "178 0.16998863\n",
      "179 0.16672032\n",
      "180 0.16351484\n",
      "181 0.1603709\n",
      "182 0.1572874\n",
      "183 0.154263\n",
      "184 0.15129675\n",
      "185 0.14838786\n",
      "186 0.14553465\n",
      "187 0.14273621\n",
      "188 0.13999164\n",
      "189 0.13730001\n",
      "190 0.1346597\n",
      "191 0.1320704\n",
      "192 0.12953098\n",
      "193 0.12704046\n",
      "194 0.124597706\n",
      "195 0.1222021\n",
      "196 0.119852595\n",
      "197 0.117548\n",
      "198 0.11528758\n",
      "199 0.113071\n",
      "200 0.11089679\n",
      "201 0.10876448\n",
      "202 0.10667303\n",
      "203 0.104622126\n",
      "204 0.10261036\n",
      "205 0.10063747\n",
      "206 0.09870234\n",
      "207 0.0968047\n",
      "208 0.094943486\n",
      "209 0.09311783\n",
      "210 0.09132749\n",
      "211 0.08957142\n",
      "212 0.08784924\n",
      "213 0.086160146\n",
      "214 0.08450339\n",
      "215 0.0828785\n",
      "216 0.081284754\n",
      "217 0.079722\n",
      "218 0.07818914\n",
      "219 0.0766857\n",
      "220 0.07521117\n",
      "221 0.07376516\n",
      "222 0.07234672\n",
      "223 0.07095583\n",
      "224 0.06959137\n",
      "225 0.06825316\n",
      "226 0.066940784\n",
      "227 0.06565376\n",
      "228 0.064391404\n",
      "229 0.063153334\n",
      "230 0.06193897\n",
      "231 0.060747772\n",
      "232 0.059579775\n",
      "233 0.058434367\n",
      "234 0.057310667\n",
      "235 0.056208774\n",
      "236 0.055127967\n",
      "237 0.054068148\n",
      "238 0.05302866\n",
      "239 0.052009072\n",
      "240 0.051008847\n",
      "241 0.050028186\n",
      "242 0.049066324\n",
      "243 0.04812273\n",
      "244 0.04719746\n",
      "245 0.046290014\n",
      "246 0.045400072\n",
      "247 0.044527162\n",
      "248 0.043671083\n",
      "249 0.042831402\n",
      "250 0.042007793\n",
      "251 0.041200116\n",
      "252 0.040407795\n",
      "253 0.039630856\n",
      "254 0.038868736\n",
      "255 0.038121436\n",
      "256 0.037388384\n",
      "257 0.036669485\n",
      "258 0.035964444\n",
      "259 0.035272833\n",
      "260 0.034594625\n",
      "261 0.033929434\n",
      "262 0.033276863\n",
      "263 0.032637097\n",
      "264 0.03200954\n",
      "265 0.031394057\n",
      "266 0.030790566\n",
      "267 0.030198509\n",
      "268 0.029617937\n",
      "269 0.029048493\n",
      "270 0.02848994\n",
      "271 0.02794207\n",
      "272 0.027404832\n",
      "273 0.026877888\n",
      "274 0.026361117\n",
      "275 0.025854252\n",
      "276 0.025357163\n",
      "277 0.024869671\n",
      "278 0.024391456\n",
      "279 0.023922483\n",
      "280 0.02346248\n",
      "281 0.023011385\n",
      "282 0.022568902\n",
      "283 0.022135025\n",
      "284 0.021709492\n",
      "285 0.021292\n",
      "286 0.020882545\n",
      "287 0.020480895\n",
      "288 0.020087168\n",
      "289 0.019700913\n",
      "290 0.019322217\n",
      "291 0.01895065\n",
      "292 0.0185863\n",
      "293 0.018228976\n",
      "294 0.017878557\n",
      "295 0.017534677\n",
      "296 0.017197507\n",
      "297 0.016866764\n",
      "298 0.016542437\n",
      "299 0.016224414\n",
      "300 0.015912408\n",
      "301 0.015606503\n",
      "302 0.015306399\n",
      "303 0.0150121115\n",
      "304 0.014723497\n",
      "305 0.014440436\n",
      "306 0.014162808\n",
      "307 0.013890429\n",
      "308 0.01362333\n",
      "309 0.01336138\n",
      "310 0.013104487\n",
      "311 0.012852456\n",
      "312 0.012605441\n",
      "313 0.01236299\n",
      "314 0.012125276\n",
      "315 0.011892214\n",
      "316 0.011663568\n",
      "317 0.011439289\n",
      "318 0.011219397\n",
      "319 0.01100362\n",
      "320 0.01079212\n",
      "321 0.010584596\n",
      "322 0.01038107\n",
      "323 0.010181485\n",
      "324 0.009985689\n",
      "325 0.009793721\n",
      "326 0.009605403\n",
      "327 0.009420668\n",
      "328 0.009239522\n",
      "329 0.009061883\n",
      "330 0.0088877035\n",
      "331 0.008716728\n",
      "332 0.008549152\n",
      "333 0.008384745\n",
      "334 0.008223501\n",
      "335 0.0080653485\n",
      "336 0.007910323\n",
      "337 0.0077581904\n",
      "338 0.007609027\n",
      "339 0.0074627385\n",
      "340 0.007319283\n",
      "341 0.007178527\n",
      "342 0.0070405747\n",
      "343 0.00690514\n",
      "344 0.0067724288\n",
      "345 0.006642224\n",
      "346 0.006514477\n",
      "347 0.006389205\n",
      "348 0.0062663318\n",
      "349 0.0061458\n",
      "350 0.006027683\n",
      "351 0.0059117563\n",
      "352 0.0057980684\n",
      "353 0.005686648\n",
      "354 0.0055772853\n",
      "355 0.005470011\n",
      "356 0.0053648255\n",
      "357 0.005261635\n",
      "358 0.0051604863\n",
      "359 0.0050612516\n",
      "360 0.004963966\n",
      "361 0.004868506\n",
      "362 0.0047748126\n",
      "363 0.00468302\n",
      "364 0.004593048\n",
      "365 0.004504649\n",
      "366 0.004418072\n",
      "367 0.004333073\n",
      "368 0.0042497828\n",
      "369 0.004168005\n",
      "370 0.0040878584\n",
      "371 0.0040092296\n",
      "372 0.0039321654\n",
      "373 0.0038565744\n",
      "374 0.0037823978\n",
      "375 0.003709681\n",
      "376 0.0036383744\n",
      "377 0.0035684481\n",
      "378 0.003499855\n",
      "379 0.0034325214\n",
      "380 0.0033664834\n",
      "381 0.003301767\n",
      "382 0.0032382635\n",
      "383 0.003176043\n",
      "384 0.0031149655\n",
      "385 0.0030550498\n",
      "386 0.0029962696\n",
      "387 0.002938635\n",
      "388 0.0028821202\n",
      "389 0.002826701\n",
      "390 0.0027723692\n",
      "391 0.0027190277\n",
      "392 0.0026667754\n",
      "393 0.0026154993\n",
      "394 0.0025651874\n",
      "395 0.0025158722\n",
      "396 0.002467537\n",
      "397 0.0024200864\n",
      "398 0.002373536\n",
      "399 0.002327888\n",
      "400 0.0022831375\n",
      "401 0.0022392191\n",
      "402 0.0021961946\n",
      "403 0.0021539417\n",
      "404 0.0021125365\n",
      "405 0.0020719154\n",
      "406 0.0020320753\n",
      "407 0.0019929972\n",
      "408 0.001954698\n",
      "409 0.0019171179\n",
      "410 0.0018802673\n",
      "411 0.001844083\n",
      "412 0.0018086595\n",
      "413 0.0017738558\n",
      "414 0.0017397412\n",
      "415 0.0017062734\n",
      "416 0.0016734934\n",
      "417 0.001641309\n",
      "418 0.0016097496\n",
      "419 0.0015788241\n",
      "420 0.0015484566\n",
      "421 0.0015186877\n",
      "422 0.0014895201\n",
      "423 0.0014608558\n",
      "424 0.001432782\n",
      "425 0.0014052265\n",
      "426 0.0013781929\n",
      "427 0.001351708\n",
      "428 0.0013257293\n",
      "429 0.0013002221\n",
      "430 0.0012752292\n",
      "431 0.0012507325\n",
      "432 0.0012266864\n",
      "433 0.0012031113\n",
      "434 0.0011799843\n",
      "435 0.0011573044\n",
      "436 0.0011350439\n",
      "437 0.0011132174\n",
      "438 0.0010918145\n",
      "439 0.0010708283\n",
      "440 0.0010502274\n",
      "441 0.0010300609\n",
      "442 0.001010249\n",
      "443 0.0009908061\n",
      "444 0.00097175577\n",
      "445 0.0009530733\n",
      "446 0.00093473896\n",
      "447 0.000916766\n",
      "448 0.0008991177\n",
      "449 0.0008818302\n",
      "450 0.0008648525\n",
      "451 0.00084823486\n",
      "452 0.0008319044\n",
      "453 0.00081592385\n",
      "454 0.0008002436\n",
      "455 0.0007848554\n",
      "456 0.0007697505\n",
      "457 0.00075495074\n",
      "458 0.0007404342\n",
      "459 0.00072618085\n",
      "460 0.0007122276\n",
      "461 0.00069854176\n",
      "462 0.00068510295\n",
      "463 0.00067193573\n",
      "464 0.0006590127\n",
      "465 0.0006463335\n",
      "466 0.0006339029\n",
      "467 0.0006217176\n",
      "468 0.0006097466\n",
      "469 0.00059801375\n",
      "470 0.00058651593\n",
      "471 0.0005752414\n",
      "472 0.0005641946\n",
      "473 0.00055335433\n",
      "474 0.00054270605\n",
      "475 0.00053227606\n",
      "476 0.0005220366\n",
      "477 0.0005120127\n",
      "478 0.00050217315\n",
      "479 0.0004925121\n",
      "480 0.00048304422\n",
      "481 0.00047375963\n",
      "482 0.00046463587\n",
      "483 0.00045571392\n",
      "484 0.00044695075\n",
      "485 0.00043834766\n",
      "486 0.0004299153\n",
      "487 0.00042165138\n",
      "488 0.0004135405\n",
      "489 0.000405593\n",
      "490 0.00039780326\n",
      "491 0.00039015384\n",
      "492 0.00038265184\n",
      "493 0.00037530996\n",
      "494 0.00036809943\n",
      "495 0.00036101474\n",
      "496 0.00035406635\n",
      "497 0.0003472638\n",
      "498 0.00034057916\n",
      "499 0.00033405036\n",
      "500 0.0003276187\n",
      "501 0.0003213192\n",
      "502 0.00031513354\n",
      "503 0.0003090684\n",
      "504 0.0003031216\n",
      "505 0.0002972971\n",
      "506 0.00029156907\n",
      "507 0.00028596004\n",
      "508 0.00028046532\n",
      "509 0.00027507823\n",
      "510 0.00026978945\n",
      "511 0.0002646026\n",
      "512 0.0002595189\n",
      "513 0.00025453157\n",
      "514 0.00024963947\n",
      "515 0.00024483874\n",
      "516 0.0002401281\n",
      "517 0.00023550626\n",
      "518 0.0002309766\n",
      "519 0.00022652133\n",
      "520 0.00022216998\n",
      "521 0.0002178998\n",
      "522 0.00021370528\n",
      "523 0.00020958747\n",
      "524 0.00020554794\n",
      "525 0.0002015984\n",
      "526 0.00019772664\n",
      "527 0.00019392505\n",
      "528 0.00019019462\n",
      "529 0.0001865345\n",
      "530 0.00018294789\n",
      "531 0.00017942935\n",
      "532 0.00017598033\n",
      "533 0.00017259941\n",
      "534 0.00016928778\n",
      "535 0.00016602226\n",
      "536 0.00016282841\n",
      "537 0.00015970487\n",
      "538 0.00015663098\n",
      "539 0.0001536257\n",
      "540 0.00015066682\n",
      "541 0.00014777624\n",
      "542 0.00014493077\n",
      "543 0.00014214807\n",
      "544 0.0001394126\n",
      "545 0.00013672933\n",
      "546 0.00013410444\n",
      "547 0.0001315301\n",
      "548 0.00012900196\n",
      "549 0.00012651057\n",
      "550 0.0001240817\n",
      "551 0.00012169008\n",
      "552 0.00011934901\n",
      "553 0.00011705921\n",
      "554 0.00011480999\n",
      "555 0.00011260906\n",
      "556 0.00011044574\n",
      "557 0.000108326174\n",
      "558 0.00010623522\n",
      "559 0.00010419323\n",
      "560 0.000102194615\n",
      "561 0.0001002232\n",
      "562 9.830031e-05\n",
      "563 9.640672e-05\n",
      "564 9.4548355e-05\n",
      "565 9.2727445e-05\n",
      "566 9.094804e-05\n",
      "567 8.9200505e-05\n",
      "568 8.7490414e-05\n",
      "569 8.580694e-05\n",
      "570 8.415992e-05\n",
      "571 8.254141e-05\n",
      "572 8.095833e-05\n",
      "573 7.939342e-05\n",
      "574 7.7869896e-05\n",
      "575 7.638137e-05\n",
      "576 7.491143e-05\n",
      "577 7.346511e-05\n",
      "578 7.205798e-05\n",
      "579 7.066963e-05\n",
      "580 6.93065e-05\n",
      "581 6.7978515e-05\n",
      "582 6.666462e-05\n",
      "583 6.538372e-05\n",
      "584 6.412522e-05\n",
      "585 6.2894986e-05\n",
      "586 6.168521e-05\n",
      "587 6.0501738e-05\n",
      "588 5.9339378e-05\n",
      "589 5.81966e-05\n",
      "590 5.7083704e-05\n",
      "591 5.598395e-05\n",
      "592 5.490751e-05\n",
      "593 5.3857584e-05\n",
      "594 5.281891e-05\n",
      "595 5.180943e-05\n",
      "596 5.0807485e-05\n",
      "597 4.9830614e-05\n",
      "598 4.887316e-05\n",
      "599 4.7931364e-05\n",
      "600 4.7014735e-05\n",
      "601 4.6107114e-05\n",
      "602 4.5217635e-05\n",
      "603 4.4353437e-05\n",
      "604 4.3499633e-05\n",
      "605 4.266023e-05\n",
      "606 4.184016e-05\n",
      "607 4.103693e-05\n",
      "608 4.024447e-05\n",
      "609 3.9476403e-05\n",
      "610 3.8716684e-05\n",
      "611 3.797297e-05\n",
      "612 3.724129e-05\n",
      "613 3.6530746e-05\n",
      "614 3.582527e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615 3.514141e-05\n",
      "616 3.4460518e-05\n",
      "617 3.3798893e-05\n",
      "618 3.314823e-05\n",
      "619 3.2512737e-05\n",
      "620 3.188605e-05\n",
      "621 3.1273292e-05\n",
      "622 3.067511e-05\n",
      "623 3.0081865e-05\n",
      "624 2.9502016e-05\n",
      "625 2.8933697e-05\n",
      "626 2.8376662e-05\n",
      "627 2.7827526e-05\n",
      "628 2.729595e-05\n",
      "629 2.6773581e-05\n",
      "630 2.6261803e-05\n",
      "631 2.575735e-05\n",
      "632 2.526176e-05\n",
      "633 2.4780189e-05\n",
      "634 2.4305582e-05\n",
      "635 2.3838635e-05\n",
      "636 2.338375e-05\n",
      "637 2.2932518e-05\n",
      "638 2.2495975e-05\n",
      "639 2.2059257e-05\n",
      "640 2.1638392e-05\n",
      "641 2.1218739e-05\n",
      "642 2.0807462e-05\n",
      "643 2.0406529e-05\n",
      "644 2.0016467e-05\n",
      "645 1.9629495e-05\n",
      "646 1.925244e-05\n",
      "647 1.8882414e-05\n",
      "648 1.852137e-05\n",
      "649 1.8168423e-05\n",
      "650 1.781691e-05\n",
      "651 1.7472741e-05\n",
      "652 1.7137723e-05\n",
      "653 1.6809174e-05\n",
      "654 1.6488204e-05\n",
      "655 1.6172218e-05\n",
      "656 1.5860509e-05\n",
      "657 1.5555563e-05\n",
      "658 1.5257188e-05\n",
      "659 1.4962337e-05\n",
      "660 1.4673315e-05\n",
      "661 1.4392425e-05\n",
      "662 1.4114307e-05\n",
      "663 1.3844074e-05\n",
      "664 1.3578166e-05\n",
      "665 1.3313736e-05\n",
      "666 1.3056393e-05\n",
      "667 1.2810465e-05\n",
      "668 1.2561369e-05\n",
      "669 1.2318e-05\n",
      "670 1.2082944e-05\n",
      "671 1.1852318e-05\n",
      "672 1.1622326e-05\n",
      "673 1.1401419e-05\n",
      "674 1.1183678e-05\n",
      "675 1.0967508e-05\n",
      "676 1.0756514e-05\n",
      "677 1.0550607e-05\n",
      "678 1.0349205e-05\n",
      "679 1.0150231e-05\n",
      "680 9.95319e-06\n",
      "681 9.761002e-06\n",
      "682 9.575021e-06\n",
      "683 9.387035e-06\n",
      "684 9.207051e-06\n",
      "685 9.03067e-06\n",
      "686 8.857383e-06\n",
      "687 8.68806e-06\n",
      "688 8.520373e-06\n",
      "689 8.3547875e-06\n",
      "690 8.197051e-06\n",
      "691 8.040391e-06\n",
      "692 7.88481e-06\n",
      "693 7.732901e-06\n",
      "694 7.584198e-06\n",
      "695 7.437777e-06\n",
      "696 7.2953085e-06\n",
      "697 7.1529917e-06\n",
      "698 7.017003e-06\n",
      "699 6.8835493e-06\n",
      "700 6.750164e-06\n",
      "701 6.62049e-06\n",
      "702 6.4940455e-06\n",
      "703 6.370405e-06\n",
      "704 6.2467866e-06\n",
      "705 6.127472e-06\n",
      "706 6.0104353e-06\n",
      "707 5.8949167e-06\n",
      "708 5.781643e-06\n",
      "709 5.672426e-06\n",
      "710 5.5624246e-06\n",
      "711 5.455684e-06\n",
      "712 5.352496e-06\n",
      "713 5.2488704e-06\n",
      "714 5.1473194e-06\n",
      "715 5.0474487e-06\n",
      "716 4.950638e-06\n",
      "717 4.854765e-06\n",
      "718 4.76187e-06\n",
      "719 4.6708833e-06\n",
      "720 4.5817746e-06\n",
      "721 4.4941885e-06\n",
      "722 4.40679e-06\n",
      "723 4.322194e-06\n",
      "724 4.2384177e-06\n",
      "725 4.1583216e-06\n",
      "726 4.078045e-06\n",
      "727 3.998551e-06\n",
      "728 3.9216916e-06\n",
      "729 3.845578e-06\n",
      "730 3.7735426e-06\n",
      "731 3.7006866e-06\n",
      "732 3.6276476e-06\n",
      "733 3.5585651e-06\n",
      "734 3.490147e-06\n",
      "735 3.4226857e-06\n",
      "736 3.357307e-06\n",
      "737 3.2934101e-06\n",
      "738 3.2295675e-06\n",
      "739 3.1677337e-06\n",
      "740 3.107047e-06\n",
      "741 3.0466774e-06\n",
      "742 2.9882456e-06\n",
      "743 2.9303799e-06\n",
      "744 2.8741404e-06\n",
      "745 2.8189654e-06\n",
      "746 2.7643257e-06\n",
      "747 2.710478e-06\n",
      "748 2.6584296e-06\n",
      "749 2.6068865e-06\n",
      "750 2.5560973e-06\n",
      "751 2.5082775e-06\n",
      "752 2.4589472e-06\n",
      "753 2.4115586e-06\n",
      "754 2.3658304e-06\n",
      "755 2.3193531e-06\n",
      "756 2.2747465e-06\n",
      "757 2.231739e-06\n",
      "758 2.187988e-06\n",
      "759 2.1467279e-06\n",
      "760 2.1063117e-06\n",
      "761 2.0651578e-06\n",
      "762 2.02441e-06\n",
      "763 1.9864863e-06\n",
      "764 1.9482607e-06\n",
      "765 1.9099807e-06\n",
      "766 1.8744298e-06\n",
      "767 1.8381528e-06\n",
      "768 1.8028626e-06\n",
      "769 1.7683291e-06\n",
      "770 1.73433e-06\n",
      "771 1.7006614e-06\n",
      "772 1.667323e-06\n",
      "773 1.6353157e-06\n",
      "774 1.6038111e-06\n",
      "775 1.5726133e-06\n",
      "776 1.5426946e-06\n",
      "777 1.5130596e-06\n",
      "778 1.4842786e-06\n",
      "779 1.4554004e-06\n",
      "780 1.4271764e-06\n",
      "781 1.3992291e-06\n",
      "782 1.3726534e-06\n",
      "783 1.3463329e-06\n",
      "784 1.3202676e-06\n",
      "785 1.2944578e-06\n",
      "786 1.2699555e-06\n",
      "787 1.2456879e-06\n",
      "788 1.2221774e-06\n",
      "789 1.1983735e-06\n",
      "790 1.1758169e-06\n",
      "791 1.1534748e-06\n",
      "792 1.1313474e-06\n",
      "793 1.1094347e-06\n",
      "794 1.0877366e-06\n",
      "795 1.0675313e-06\n",
      "796 1.047205e-06\n",
      "797 1.0270743e-06\n",
      "798 1.0071393e-06\n",
      "799 9.878712e-07\n",
      "800 9.692423e-07\n",
      "801 9.5079105e-07\n",
      "802 9.325175e-07\n",
      "803 9.144216e-07\n",
      "804 8.965033e-07\n",
      "805 8.796385e-07\n",
      "806 8.629342e-07\n",
      "807 8.463904e-07\n",
      "808 8.3000714e-07\n",
      "809 8.137844e-07\n",
      "810 7.977222e-07\n",
      "811 7.8306994e-07\n",
      "812 7.6840735e-07\n",
      "813 7.53459e-07\n",
      "814 7.3907864e-07\n",
      "815 7.248378e-07\n",
      "816 7.112594e-07\n",
      "817 6.974105e-07\n",
      "818 6.843523e-07\n",
      "819 6.714183e-07\n",
      "820 6.579655e-07\n",
      "821 6.452855e-07\n",
      "822 6.334724e-07\n",
      "823 6.211433e-07\n",
      "824 6.095551e-07\n",
      "825 5.9807667e-07\n",
      "826 5.8609925e-07\n",
      "827 5.748457e-07\n",
      "828 5.640604e-07\n",
      "829 5.5312466e-07\n",
      "830 5.4288245e-07\n",
      "831 5.327365e-07\n",
      "832 5.2211084e-07\n",
      "833 5.1216256e-07\n",
      "834 5.0174543e-07\n",
      "835 4.919948e-07\n",
      "836 4.8298836e-07\n",
      "837 4.73516e-07\n",
      "838 4.6468176e-07\n",
      "839 4.559312e-07\n",
      "840 4.4692607e-07\n",
      "841 4.383456e-07\n",
      "842 4.298488e-07\n",
      "843 4.2152155e-07\n",
      "844 4.1379045e-07\n",
      "845 4.0643758e-07\n",
      "846 3.983424e-07\n",
      "847 3.9082911e-07\n",
      "848 3.8338786e-07\n",
      "849 3.755275e-07\n",
      "850 3.6823477e-07\n",
      "851 3.6157454e-07\n",
      "852 3.5449807e-07\n",
      "853 3.4796457e-07\n",
      "854 3.414923e-07\n",
      "855 3.346171e-07\n",
      "856 3.2827157e-07\n",
      "857 3.2198727e-07\n",
      "858 3.153134e-07\n",
      "859 3.091558e-07\n",
      "860 3.035731e-07\n",
      "861 2.977753e-07\n",
      "862 2.922975e-07\n",
      "863 2.8687109e-07\n",
      "864 2.8123654e-07\n",
      "865 2.7608112e-07\n",
      "866 2.703902e-07\n",
      "867 2.6533667e-07\n",
      "868 2.599204e-07\n",
      "869 2.554209e-07\n",
      "870 2.505739e-07\n",
      "871 2.4577358e-07\n",
      "872 2.414171e-07\n",
      "873 2.365507e-07\n",
      "874 2.322779e-07\n",
      "875 2.2765828e-07\n",
      "876 2.2308534e-07\n",
      "877 2.1893864e-07\n",
      "878 2.1430567e-07\n",
      "879 2.106696e-07\n",
      "880 2.0669563e-07\n",
      "881 2.0275972e-07\n",
      "882 1.9922504e-07\n",
      "883 1.9521819e-07\n",
      "884 1.9139429e-07\n",
      "885 1.8796163e-07\n",
      "886 1.8421049e-07\n",
      "887 1.8049741e-07\n",
      "888 1.7716616e-07\n",
      "889 1.7352583e-07\n",
      "890 1.6992355e-07\n",
      "891 1.670732e-07\n",
      "892 1.6391543e-07\n",
      "893 1.6098443e-07\n",
      "894 1.5821168e-07\n",
      "895 1.5514001e-07\n",
      "896 1.5209866e-07\n",
      "897 1.4940535e-07\n",
      "898 1.464216e-07\n",
      "899 1.4346817e-07\n",
      "900 1.4085428e-07\n",
      "901 1.3795845e-07\n",
      "902 1.3509293e-07\n",
      "903 1.3289628e-07\n",
      "904 1.3041934e-07\n",
      "905 1.279659e-07\n",
      "906 1.2582943e-07\n",
      "907 1.2342025e-07\n",
      "908 1.2103457e-07\n",
      "909 1.1895827e-07\n",
      "910 1.1661685e-07\n",
      "911 1.1429893e-07\n",
      "912 1.12282805e-07\n",
      "913 1.1000915e-07\n",
      "914 1.0775898e-07\n",
      "915 1.0580303e-07\n",
      "916 1.0359713e-07\n",
      "917 1.0141472e-07\n",
      "918 9.981098e-08\n",
      "919 9.7958946e-08\n",
      "920 9.620491e-08\n",
      "921 9.4643916e-08\n",
      "922 9.28413e-08\n",
      "923 9.105626e-08\n",
      "924 8.953892e-08\n",
      "925 8.7786624e-08\n",
      "926 8.605192e-08\n",
      "927 8.457823e-08\n",
      "928 8.302087e-08\n",
      "929 8.133528e-08\n",
      "930 7.9904034e-08\n",
      "931 7.825118e-08\n",
      "932 7.661591e-08\n",
      "933 7.522832e-08\n",
      "934 7.362579e-08\n",
      "935 7.204085e-08\n",
      "936 7.0942555e-08\n",
      "937 6.963099e-08\n",
      "938 6.833201e-08\n",
      "939 6.726358e-08\n",
      "940 6.598763e-08\n",
      "941 6.472427e-08\n",
      "942 6.36857e-08\n",
      "943 6.244537e-08\n",
      "944 6.1217634e-08\n",
      "945 6.0208926e-08\n",
      "946 5.9004226e-08\n",
      "947 5.7812105e-08\n",
      "948 5.6708966e-08\n",
      "949 5.5739974e-08\n",
      "950 5.4582262e-08\n",
      "951 5.3511712e-08\n",
      "952 5.2377953e-08\n",
      "953 5.1448676e-08\n",
      "954 5.0410716e-08\n",
      "955 4.9311364e-08\n",
      "956 4.8426728e-08\n",
      "957 4.7620915e-08\n",
      "958 4.682238e-08\n",
      "959 4.6143594e-08\n",
      "960 4.528867e-08\n",
      "961 4.4510745e-08\n",
      "962 4.3671587e-08\n",
      "963 4.2908216e-08\n",
      "964 4.2152124e-08\n",
      "965 4.1510322e-08\n",
      "966 4.0700872e-08\n",
      "967 3.9965396e-08\n",
      "968 3.917171e-08\n",
      "969 3.8450782e-08\n",
      "Gradient of m : 3.30\n",
      "Gradient of c : 5.30\n"
     ]
    }
   ],
   "source": [
    "X = tf.convert_to_tensor([-0.0374,  2.6822, -4.1152])\n",
    "Y = tf.convert_to_tensor([ 5.1765, 14.1513, -8.2802])\n",
    "m = 2\n",
    "c = 3\n",
    "k = tf.convert_to_tensor([0, 2])\n",
    "\n",
    "gm, gc = gradient_wrt_m_and_c(X, Y, m, c, k)\n",
    "\n",
    "print(f'Gradient of m : {gm:.2f}')\n",
    "print(f'Gradient of c : {gc:.2f}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T09:18:28.103247Z",
     "start_time": "2021-12-22T09:18:27.997254Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Implement gradients wrt m",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T09:18:28.134255Z",
     "start_time": "2021-12-22T09:18:28.106241Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Implement gradients wrt c",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### 2.2 Stochastic Gradient Descent (SGD) [10 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T09:18:28.150242Z",
     "start_time": "2021-12-22T09:18:28.137241Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_m_and_c(m, c, g_m, g_c, lr):\n",
    "    '''\n",
    "    All arguments are defined in the training section of this notebook. \n",
    "    This function will be called from the training section.  \n",
    "    So before completing this function go through the whole notebook.\n",
    "    \n",
    "    g_m = gradient w.r.t to m\n",
    "    c_m = gradient w.r.t to c\n",
    "    '''\n",
    "    # Update m and c parameters.\n",
    "    # store updated value of m is updated_m variable\n",
    "    # store updated value of c is updated_c variable\n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "    ###\n",
    "    return updated_m, updated_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Test your code before submitting it using the below code cell.**\n",
    "\n",
    "For the given input:\n",
    "```\n",
    "m = 2\n",
    "c = 3\n",
    "g_m = -24.93\n",
    "g_c = 1.60\n",
    "lr = 0.001\n",
    "```\n",
    "Output:\n",
    "```\n",
    "Updated m: 2.02\n",
    "Updated c: 3.00\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T09:18:28.166242Z",
     "start_time": "2021-12-22T09:18:28.154245Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "m = 2\n",
    "c = 3\n",
    "g_m = -24.93\n",
    "g_c = 1.60\n",
    "lr = 0.001\n",
    "m, c = update_m_and_c(m, c, g_m, g_c, lr)\n",
    "\n",
    "print('Updated m: {0:.2f}'.format(m))\n",
    "print('Updated c: {0:.2f}'.format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T09:18:28.182241Z",
     "start_time": "2021-12-22T09:18:28.170242Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "SGD update of m",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T09:18:28.198245Z",
     "start_time": "2021-12-22T09:18:28.186239Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "SGD update of c",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## 3  Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T09:18:35.423454Z",
     "start_time": "2021-12-22T09:18:28.203243Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent with Minibatch.\n",
    "\n",
    "# Input \n",
    "X = x\n",
    "\n",
    "# output label.\n",
    "Y = y_label\n",
    "\n",
    "num_iter = 1000\n",
    "batch_size = 10\n",
    "\n",
    "# display updated values after every 50 iterations.\n",
    "display_count = 50\n",
    "# \n",
    "\n",
    "lr = 0.001\n",
    "m = 2\n",
    "c = 1\n",
    "print()\n",
    "loss = []\n",
    "\n",
    "for i in range(0, num_iter):\n",
    "\n",
    "    # Randomly select a training data point.\n",
    "    k = tf.random.uniform(shape=[batch_size], minval=0, maxval=len(Y)-1, dtype=tf.int32)\n",
    "\n",
    "    # Calculate gradient of m and c using a mini-batch.\n",
    "    g_m, g_c = gradient_wrt_m_and_c(X, Y, m, c, k)\n",
    "    \n",
    "    # update m and c parameters.\n",
    "    m, c = update_m_and_c(m, c, g_m, g_c, lr)\n",
    "    \n",
    "    # Calculate Error.\n",
    "    e = Y - m * X - c\n",
    "    \n",
    "    # Compute Loss Function.\n",
    "    current_loss = tf.math.reduce_sum(tf.math.multiply(e,e))\n",
    "    loss.append(current_loss)\n",
    "\n",
    "    if i % display_count==0:\n",
    "        print('Iteration: {}, Loss: {}, updated m: {:.3f}, updated c: {:.3f}'.format(i, loss[i], m, c))\n",
    "        y_pred = m * X + c\n",
    "        # Plot the line corresponding to the learned m and c.\n",
    "        plt.plot(x, y_label, '.', color='g')\n",
    "        plt.plot(x, y, color='b', label='Line corresponding to m={0:.2f}, c={1:.2f}'.\n",
    "                 format(m_line, c_line), linewidth=3)\n",
    "        plt.plot(X, y_pred, color='r', label='Line corresponding to m_learned={0:.2f}, c_learned={1:.2f}'.\n",
    "                 format(m, c), linewidth=3)\n",
    "        plt.title(\"Iteration : {}\".format(i))\n",
    "        plt.legend()\n",
    "\n",
    "        plt.ylabel('y')\n",
    "        plt.xlabel('x')\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T09:18:35.755447Z",
     "start_time": "2021-12-22T09:18:35.425454Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Loss of after last batch: {}'.format(loss[-1]))\n",
    "print('Leaned \"m\" value: {}'.format( m))\n",
    "print('Leaned \"c\" value: {}'.format( c))\n",
    "\n",
    "# Plot loss vs m.\n",
    "plt.figure\n",
    "plt.plot(range(len(loss)),loss)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T09:18:35.771450Z",
     "start_time": "2021-12-22T09:18:35.757449Z"
    },
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 0,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the predicted y values using the learned m and c.\n",
    "y_pred = m * X + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T09:18:36.007447Z",
     "start_time": "2021-12-22T09:18:35.774452Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the line corresponding to the learned m and c\n",
    "plt.plot(x, y_label, '.', color='g', label='X and Y')\n",
    "plt.plot(x, y, color='b', label='Line corresponding to m={0:.2f}, c={1:.2f}'.format(m_line, c_line), linewidth=3)\n",
    "plt.plot(X, y_pred, color='r', label='Line corresponding to m_learned={0:.2f}, c_learned={1:.2f}'.format(m, c), linewidth=3)\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": [],
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": [],
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
